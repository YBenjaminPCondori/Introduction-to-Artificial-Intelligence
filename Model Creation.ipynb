{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f0b45f",
   "metadata": {},
   "source": [
    "# # London Crime MLP Classifier\n",
    "# Predicting **Crime type** using TensorFlow/Keras Multi-Layer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d153e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Conv1D, GlobalMaxPooling1D, Flatten, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95dca2",
   "metadata": {},
   "source": [
    "Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca51c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, epoch_label, save_path=\"Model\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save(os.path.join(save_path, f\"model_epoch_{epoch_label}.h5\"))\n",
    "    model.save_weights(os.path.join(save_path, f\"weights_epoch_{epoch_label}.h5\"))\n",
    "    print(f\"[✓] Saved model at epoch {epoch_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097aff64",
   "metadata": {},
   "source": [
    "# ## GPU Acceleration Pre-Training Hardware Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) configured: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU\")\n",
    "\n",
    "# Disable mixed precision entirely (float32 is safest)\n",
    "mixed_precision.set_global_policy('float32')\n",
    "\n",
    "print(\"Mixed precision disabled. Using float32 for all ops.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4bcb2",
   "metadata": {},
   "source": [
    "# ## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaec833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Memory-Efficient Data Preprocessing (Chunked CSV → HDF5)\n",
    "input_csv = \"Dataset/mergedCRDataset.csv\"\n",
    "preprocessed_hdf5_file = \"Dataset/Preprocessed/london_crime_data.h5\"\n",
    "os.makedirs(os.path.dirname(preprocessed_hdf5_file), exist_ok=True)\n",
    "\n",
    "chunksize = 1_000_000\n",
    "chunk_idx = 0\n",
    "\n",
    "numeric_cols = ['Longitude', 'Latitude']\n",
    "categorical_cols = ['Reported by', 'Falls within']\n",
    "target_col = 'Crime type'\n",
    "group_col = 'LSOA name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f2baa",
   "metadata": {},
   "source": [
    "# ## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c271737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an HDF5 file to store the processed data\n",
    "with h5py.File(preprocessed_hdf5_file, 'w') as hdf5_file:\n",
    "    # Create datasets for storing features and targets\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    \n",
    "    for chunk in pd.read_csv(input_csv, sep=\"\\t\", engine=\"python\", on_bad_lines=\"skip\", chunksize=chunksize):\n",
    "        # Strip column names\n",
    "        chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "        # --- Print summary for this chunk ---\n",
    "        summary = pd.DataFrame({\n",
    "            \"missing\": chunk.isna().sum(),\n",
    "            \"non_missing\": chunk.notna().sum(),\n",
    "            \"total\": len(chunk)\n",
    "        })\n",
    "        print(f\"\\nChunk {chunk_idx} summary:\")\n",
    "        print(summary)\n",
    "\n",
    "        # Drop duplicates and unnecessary columns\n",
    "        chunk.drop_duplicates(inplace=True)\n",
    "        if \"Crime ID\" in chunk.columns:\n",
    "            chunk.drop(columns=[\"Crime ID\"], inplace=True)\n",
    "\n",
    "        # Fill numeric NaNs using group medians\n",
    "        group_medians = chunk.groupby(group_col)[numeric_cols].median()\n",
    "        chunk = chunk.merge(group_medians, left_on=group_col, right_index=True, how='left', suffixes=('', '_grp'))\n",
    "        for col in numeric_cols:\n",
    "            chunk[col] = chunk[col].fillna(chunk[f'{col}_grp'])\n",
    "        global_medians = chunk[numeric_cols].median()\n",
    "        chunk[numeric_cols] = chunk[numeric_cols].fillna(global_medians)\n",
    "        chunk.drop(columns=[f'{c}_grp' for c in numeric_cols], inplace=True)\n",
    "\n",
    "        # Encode target\n",
    "        chunk[target_col] = chunk[target_col].astype('category')\n",
    "        chunk['target_encoded'] = chunk[target_col].cat.codes\n",
    "\n",
    "        # Convert categorical features to category dtype\n",
    "        for col in categorical_cols:\n",
    "            chunk[col] = chunk[col].astype('category')\n",
    "\n",
    "        # --- Print target counts for monitoring ---\n",
    "        print(f\"Chunk {chunk_idx} target value counts:\")\n",
    "        print(chunk[target_col].value_counts())\n",
    "\n",
    "        # Prepare data for saving\n",
    "        X_chunk = chunk[numeric_cols + categorical_cols]\n",
    "        y_chunk = chunk['target_encoded']\n",
    "        \n",
    "        # Add to lists for saving later in the HDF5 file\n",
    "        X_all.append(X_chunk)\n",
    "        y_all.append(y_chunk)\n",
    "\n",
    "        chunk_idx += 1\n",
    "        \n",
    "    # Stack all chunks into a single array\n",
    "    X_all = pd.concat(X_all, ignore_index=True)\n",
    "    y_all = pd.concat(y_all, ignore_index=True)\n",
    "\n",
    "    # Convert everything to string first (prevents dtype O issues)\n",
    "    X_all = X_all.astype(str)\n",
    "\n",
    "    # Label encode each column\n",
    "    for col in X_all.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_all[col] = le.fit_transform(X_all[col])\n",
    "\n",
    "    # Convert to HDF5-safe arrays\n",
    "    X_np = X_all.to_numpy(dtype='float32')\n",
    "    y_np = y_all.to_numpy(dtype='int32')\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "\n",
    "    # Store in HDF5\n",
    "    hdf5_file.create_dataset('X', data=X_np)\n",
    "    hdf5_file.create_dataset('y', data=y_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce122c",
   "metadata": {},
   "source": [
    "# ## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e45b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Preprocessed Data, Scaling & One-Hot Encoding\n",
    "# Load data from HDF5\n",
    "with h5py.File(preprocessed_hdf5_file, 'r') as hdf5_file:\n",
    "    X = hdf5_file['X'][:]\n",
    "    y = hdf5_file['y'][:]\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X[:, :len(numeric_cols)] = scaler.fit_transform(X[:, :len(numeric_cols)])\n",
    "\n",
    "# One-hot encode target\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "num_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5464c5d",
   "metadata": {},
   "source": [
    "# ## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ce479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable multi-GPU training\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"Available GPUs:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=X_train.shape[1:]))\n",
    "\n",
    "    # Add L2 regularization to first dense layer\n",
    "    model.add(Dense(\n",
    "        64, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.001)   # <<< L2 penalty\n",
    "    ))\n",
    "\n",
    "    # Add L2 regularization to output layer\n",
    "    model.add(Dense(\n",
    "        num_classes,\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=l2(0.001)   # <<< L2 penalty\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "save_path = 'Model'\n",
    "os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c5840",
   "metadata": {},
   "source": [
    "# ## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31541e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_intervals = [10, 25]\n",
    "previous_epochs = 0\n",
    "\n",
    "stage_histories = {}   # <- save history per stage\n",
    "\n",
    "for target_epochs in epoch_intervals:\n",
    "    epochs_to_run = target_epochs - previous_epochs\n",
    "\n",
    "    print(f\"\\nTraining to reach {target_epochs} total epochs \"\n",
    "          f\"({epochs_to_run} new epochs)...\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=epochs_to_run,\n",
    "        batch_size=512,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save history for this stage\n",
    "    stage_histories[f\"up_to_{target_epochs}_epochs\"] = history.history\n",
    "\n",
    "    #\n",
    "    save_model_checkpoint(model, target_epochs, save_path=\"Model\")\n",
    "\n",
    "    # Update counter\n",
    "    previous_epochs = target_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save staged training history ---\n",
    "history_file = os.path.join(save_path, 'staged_training_history.pkl')\n",
    "with open(history_file, 'wb') as f:\n",
    "    pickle.dump(stage_histories, f)\n",
    "\n",
    "# --- Save the full model and weights ---\n",
    "model_file = os.path.join(save_path, 'my_model.h5')\n",
    "weights_file = os.path.join(save_path, 'my_model_weights.h5')\n",
    "\n",
    "model.save(model_file)\n",
    "model.save_weights(weights_file)\n",
    "\n",
    "print(f\"Staged training history saved to {history_file}\")\n",
    "print(f\"Model saved to {model_file}\")\n",
    "print(f\"Weights saved to {weights_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cbb3f",
   "metadata": {},
   "source": [
    "Evaluation Defnition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e953b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate predictions for evaluation ---\n",
    "y_probs = model.predict(X_test)                    # Probabilities\n",
    "y_pred_classes = np.argmax(y_probs, axis=1)        # Predicted labels\n",
    "y_true_classes = np.argmax(y_test, axis=1)         # True labels\n",
    "\n",
    "# For compatibility with later code:\n",
    "all_true = y_true_classes\n",
    "all_preds = y_pred_classes\n",
    "all_probs = y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496ad7d",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track loss and accuracy for visualization\n",
    "loss_across_epochs = history.history['loss']\n",
    "val_accuracy_across_epochs = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba78f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization (example for Training Loss vs Validation Accuracy)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "epochs_range = range(1, len(loss_across_epochs) + 1)\n",
    "ax_twin = ax.twinx()\n",
    "line1 = ax.plot(epochs_range, loss_across_epochs, 'b-o', label='Training Loss', linewidth=2)\n",
    "line2 = ax_twin.plot(epochs_range, val_accuracy_across_epochs, 'g-s', label='Validation Accuracy', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12, color='b')\n",
    "ax_twin.set_ylabel('Accuracy', fontsize=12, color='g')\n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "ax_twin.tick_params(axis='y', labelcolor='g')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Training Loss vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax.legend(lines, labels, loc='upper left', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig('model_training_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d65d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions (will give a probability distribution)\n",
    "pred_hot = model.predict(X_test)\n",
    "#now pick the most likely outcome\n",
    "pred = np.argmax(pred_hot,axis=1)\n",
    "y_compare = np.argmax(y_test,axis=1) \n",
    "#calculate accuracy\n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "print(pred_hot[:5])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed00f77",
   "metadata": {},
   "source": [
    "# ## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "# --- Step 2: Classification report ---\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: F1 Scores ---\n",
    "macro_f1 = f1_score(y_true_classes, y_pred_classes, average='macro', zero_division=0)\n",
    "weighted_f1 = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "print(f\"\\n1. F1 SCORES:\")\n",
    "print(f\"   Macro F1 (treats all classes equally): {macro_f1:.4f}\")\n",
    "print(f\"   Weighted F1 (accounts for class imbalance): {weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Recall Scores ---\n",
    "macro_recall = recall_score(y_true_classes, y_pred_classes, average='macro', zero_division=0)\n",
    "weighted_recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "per_class_recall = recall_score(y_true_classes, y_pred_classes, average=None, zero_division=0)\n",
    "print(f\"\\n2. RECALL SCORES (Detection Rate):\")\n",
    "print(f\"   Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"   Weighted Recall: {weighted_recall:.4f}\")\n",
    "print(f\"   Per-class Recall:\")\n",
    "for i, recall in enumerate(per_class_recall):\n",
    "    print(f\"      Class {i}: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "print(f\"\\n3. Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: AUROC / ROC-AUC ---\n",
    "try:\n",
    "    if num_classes == 2:\n",
    "        auc = roc_auc_score(y_true_classes, y_probs[:, 1])\n",
    "        print(f\"\\n4. AUROC / ROC-AUC Score:\")\n",
    "        print(f\"   Binary AUC: {auc:.4f}\")\n",
    "    else:\n",
    "        macro_auc = roc_auc_score(y_true_classes, y_probs, multi_class='ovr', average='macro')\n",
    "        weighted_auc = roc_auc_score(y_true_classes, y_probs, multi_class='ovr', average='weighted')\n",
    "        print(f\"\\n4. AUROC / ROC-AUC Score:\")\n",
    "        print(f\"   Macro AUC (One-vs-Rest): {macro_auc:.4f}\")\n",
    "        print(f\"   Weighted AUC: {weighted_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n4. AUROC / ROC-AUC:\")\n",
    "    print(f\"   Could not compute AUC: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Overall Accuracy ---\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ca166",
   "metadata": {},
   "source": [
    "# Data Analysis/Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Confusion Matrix Heatmap\n",
    "ax1 = axes[0, 0]\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. F1 Scores Comparison\n",
    "ax1 = axes[0]\n",
    "metrics = ['Macro F1', 'Weighted F1']\n",
    "scores = [macro_f1, weighted_f1]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(metrics, scores, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Scores Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Macro Metrics Summary\n",
    "ax2 = axes[1]\n",
    "macro_precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "summary_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "summary_values = [accuracy, macro_precision, macro_recall, macro_f1]\n",
    "colors_summary = ['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3']\n",
    "\n",
    "bars = ax2.barh(summary_metrics, summary_values, color=colors_summary, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro-Averaged Metrics Summary', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, value in zip(bars, summary_values):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{value:.4f}', ha='left', va='center', fontsize=11, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb94960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalized Confusion Matrix\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', ax=ax2, cbar_kws={'label': 'Recall %'})\n",
    "ax2.set_title('Confusion Matrix (Normalized by True Label)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e97531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Loss and Validation Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "epochs_range = range(1, len(loss_across_epochs) + 1)\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(epochs_range, loss_across_epochs, 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "line2 = ax3_twin.plot(epochs_range, val_accuracy_across_epochs, 'g-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12, color='b')\n",
    "ax3_twin.set_ylabel('Accuracy', fontsize=12, color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='b')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='g')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_title('Training Loss vs Validation Accuracy', fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Per-Class Recall Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "class_names = [f'Class {i}' for i in range(len(per_class_recall))]\n",
    "colors = ['green' if r > 0.5 else 'orange' if r > 0.3 else 'red' for r in per_class_recall]\n",
    "bars = ax4.bar(class_names, per_class_recall, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.axhline(y=macro_recall, color='r', linestyle='--', linewidth=2, label=f'Macro Recall: {macro_recall:.4f}')\n",
    "ax4.set_ylabel('Recall', fontsize=12)\n",
    "ax4.set_title('Per-Class Recall (Detection Rate)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fff683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add value labels on bars\n",
    "for bar, recall in zip(bars, per_class_recall):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{recall:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Curves (One-vs-Rest for multi-class)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ROC Curve (One-vs-Rest)\n",
    "ax1 = axes[0]\n",
    "if num_classes > 2:\n",
    "    # Multi-class: use label binarization\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, num_classes))\n",
    "    auc_scores = []\n",
    "    \n",
    "    for i in range(min(num_classes, 5)):  # Limit to 5 classes for clarity\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "        ax1.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (AUC = {roc_auc:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86385ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro-average\n",
    "    fpr, tpr, _ = roc_curve(all_true_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc_micro = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='deeppink', lw=3, linestyle=':', label=f'Micro-average (AUC = {roc_auc_micro:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curves (One-vs-Rest) - Top 5 Classes', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Binary classification\n",
    "    fpr, tpr, _ = roc_curve(all_true, all_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Per-Class Metrics Heatmap\n",
    "ax2 = axes[1]\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_true, all_preds, \n",
    "                                                                   average=None, zero_division=0)\n",
    "\n",
    "metrics_data = np.array([precision, recall, f1]).T\n",
    "im = ax2.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ticks and labels\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "ax2.set_xticklabels(['Precision', 'Recall', 'F1'], fontsize=11, fontweight='bold')\n",
    "ax2.set_yticks(range(min(len(precision), 10)))\n",
    "ax2.set_yticklabels([f'Class {i}' for i in range(min(len(precision), 10))], fontsize=10)\n",
    "ax2.set_title('Per-Class Metrics Heatmap (Top 10 Classes)', fontsize=13, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text annotations\n",
    "for i in range(min(len(precision), 10)):\n",
    "    for j in range(3):\n",
    "        text = ax2.text(j, i, f'{metrics_data[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "cbar.set_label('Score', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_and_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
